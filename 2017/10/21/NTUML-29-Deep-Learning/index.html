<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="NTUML,自动编码器,PCA," />










<meta name="description" content="本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！ 本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！！ 本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！！！ 深度神经网络 神经网络的核心是一层层的神经元和它们之间的连接关系。一个比较直接的问题，是应该如何设计神经网络？它应该有多少层，每一">
<meta name="keywords" content="NTUML,自动编码器,PCA">
<meta property="og:type" content="article">
<meta property="og:title" content="NTUML 29. 深度学习">
<meta property="og:url" content="txshi-mt.com/2017/10/21/NTUML-29-Deep-Learning/index.html">
<meta property="og:site_name" content="Tingxun&#39;s Blog">
<meta property="og:description" content="本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！ 本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！！ 本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！！！ 深度神经网络 神经网络的核心是一层层的神经元和它们之间的连接关系。一个比较直接的问题，是应该如何设计神经网络？它应该有多少层，每一">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://os2v1pkzv.bkt.clouddn.com/NTUML/NTUML29_dl_example.png">
<meta property="og:image" content="http://os2v1pkzv.bkt.clouddn.com/NTUML29_ac.png">
<meta property="og:updated_time" content="2017-12-19T12:35:39.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NTUML 29. 深度学习">
<meta name="twitter:description" content="本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！ 本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！！ 本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！！！ 深度神经网络 神经网络的核心是一层层的神经元和它们之间的连接关系。一个比较直接的问题，是应该如何设计神经网络？它应该有多少层，每一">
<meta name="twitter:image" content="http://os2v1pkzv.bkt.clouddn.com/NTUML/NTUML29_dl_example.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="txshi-mt.com/2017/10/21/NTUML-29-Deep-Learning/"/>





  <title>NTUML 29. 深度学习 | Tingxun's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Tingxun's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">念念不忘，必有回响</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="txshi-mt.com/2017/10/21/NTUML-29-Deep-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Tingxun Shi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tingxun's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">NTUML 29. 深度学习</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-21T17:43:37+08:00">
                Oct 21 2017
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/统计机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">统计机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/统计机器学习/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！</strong></p>
<p><strong>本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！！</strong></p>
<p><strong>本讲原标题有点标题党，只讲了一点关于深度学习的皮毛，还有一部分在讲PCA，不要被误导！！！</strong></p>
<h2 id="深度神经网络">深度神经网络</h2>
<p>神经网络的核心是一层层的神经元和它们之间的连接关系。一个比较直接的问题，是应该如何设计神经网络？它应该有多少层，每一层各自应该有多少神经元？或者更泛泛地说，应该选择什么样的网络结构？这是神经网络应用时一个非常核心也非常困难的问题。神经网络结构可以粗浅地分成两种：浅层神经网络和深度神经网络，两者区别是前者只有少量的隐藏层，而后者有很多层。浅层神经网络训练起来更有效，关于结构的决定更简单，有足够有力的理论基础。相比而言，深度学习训练起来比较难，也难以决定网络的结构。但是当层数足够多时，网络的能力也会非常强，而且它可以提取出有物理意义的特征</p>
<p>例如，现在要分辨手写的数字是1还是5，那么可以先想办法在笔记里萃取出各个部位的笔画特征，例如中间是竖，底层是不是弯等等。这些特征（笔画）可能通过组合变得更复杂，到最后一步再用来做辨识动作，如下图所示</p>
<figure>
<img src="http://os2v1pkzv.bkt.clouddn.com/NTUML/NTUML29_dl_example.png" alt="深度学习示例：手写数字识别"><figcaption>深度学习示例：手写数字识别</figcaption>
</figure>
<p>这里实际每一层都可以解读出物理意义。例如要辨识1，可能需要<span class="math inline">\(\phi_1, \phi_2, \phi_3\)</span>，不需要<span class="math inline">\(\phi_4, \phi_5, \phi_6\)</span>。这样看，第一层其实做的是从像素点提取笔画的功能，第二层则是对第一层做组合，设计不同权重，等等。层数越多，就能表示越多种不同的变化，每层要做的事情也相对来讲变得更加简单。在这种架构下，每一层只需要贡献出一份微薄的力量，做一点微小的工作，就可以使整个网络完成从简单特征到复杂特征的变换。因此，深度学习适用于那些难以从原始特征做学习的任务，例如语音处理</p>
<p>前面提到过深度学习会面临几个难题，目前（2015年）主要采取如下的方式来应对</p>
<ul>
<li>难以决定具体应使用何种网络结构：使用一些领域知识，加入对问题的了解。例如处理图像时使用卷积神经网络（Convolutional NN，CNN），这样让每层神经元都只处理一小块像素</li>
<li>模型复杂度很高：如果数据足够大，就不是问题。如果的确想控制模型复杂度，想容忍噪声，就使用一些特有的正则化方法，例如dropout（训练时随机丢弃一部分隐藏层神经元）或者降噪（denoising，后面介绍）</li>
<li>难以最优化：谨慎选择初始权重，以避免陷入差的局部最优解（称为预训练）</li>
<li>过高的计算复杂度（尤其是在大数据背景下此问题更严重）：使用新的硬件和体系结构，例如使用GPU做mini-batch</li>
</ul>
<p>本讲无法涵盖所有深度学习的技巧，只能介绍一点入门的部分。这里再多说几句，介绍一种比较简单的预训练方法。不像之前提到的神经网络那样上来对所有权重做决定，这里使用了一种分层决定的方法：对第<span class="math inline">\(\ell = 1,\ldots , L\)</span>层，假设<span class="math inline">\(w_\ast^{(1)}, \ldots, w_\ast^{(\ell -1)}\)</span>都已经固定，一层一层去对<span class="math inline">\(\{w_{ij}^{(\ell)}\}\)</span>做预训练。每层权重都预训练好了以后，再使用反向传播来调优所有的<span class="math inline">\(\{w_{ij}^{(\ell)}\}\)</span>。接下来的问题就是怎么在每一层进行预训练，以及怎么加入正则化来控制模型复杂度</p>
<h2 id="自动编码器">自动编码器</h2>
<p>前面讲过，预训练的意义是寻找一个好的初始化权重，那么什么叫“好的”初始化权重呢？这里引发了我们对权重的物理意义的思考。由前面的讲解，可知权重本质上是告知模型如何做特征变换，或者说，怎么把原始数据换一种表示形式（称为经过了一次<strong>编码</strong>）。做深度学习时，并不知道初始化（预训练得到）的权重在后面会被如何调整。既然如此，不如希望预训练后得到的权重能够（精炼地）保持原始数据的特征：传给后面神经元的数据不是乱七八糟的，而是对相同信息的一种不同表示。例如，对前面的手写数字例子，第一层提取出的笔画特征经过组合，可以还原出原始的数据。那么什么是维持原来数据的特征变换呢？如果使用变换后的数据可以轻易重建原来的数据，就说明原始数据的信息没有什么损失。也就是说，我们希望深度学习预训练时得到的特征变换（初始权重）都是能保持原始信息的</p>
<p>那么如何得到这种能保持原始信息的特征变换呢？一种做法就是仍然使用一个神经网络来解决：把原始的数据通过变换送入到隐藏层神经元以后，这些神经元的输出经过一些变换还能还原成原始的输入（与原始的输入类似）。这个神经网络<span class="math inline">\(g\)</span>有一个独特的名字，称为<strong>自动编码器</strong>，它是一个<span class="math inline">\(d-\tilde{d}-d\)</span>三层的神经网络，从输入层到隐藏层是编码操作，从隐藏层到输出层是解码操作，保证最后输出<span class="math inline">\(g_i({\bf x}) \approx {\bf x}_i\)</span>（也就是说，此网络是学习逼近一个恒等函数）。其中<span class="math inline">\(\tilde{d}\)</span>是编码的维度。整个自动编码器的结构如下图所示</p>
<figure>
<img src="http://os2v1pkzv.bkt.clouddn.com/NTUML29_ac.png" alt="自动编码器示意"><figcaption>自动编码器示意</figcaption>
</figure>
<p>其中<span class="math inline">\(w_{ij}^{(1)}\)</span>称为编码权重，<span class="math inline">\(w_{ji}^{(2)}\)</span>称为解码权重</p>
<p>那么为什么要设计这么一个复杂的结构，费劲去逼近一个恒等函数（相当于一个什么也不做的函数）？如果真的能得到<span class="math inline">\(g({\bf x}) \approx {\bf x}\)</span>，则它必然依赖了原始数据一些<em>隐藏的</em>特征/结构。如果能通过这个学习过程得到隐藏的特征/结构，那么就可以把这些特征/结构当做特征变换，用在预训练中。这些特征/结构告诉我们如何用一些有效的信息表示原始数据。对于非监督学习问题，自动编码器也有其实际意义。例如如果要做密度估计问题，则对于密度大的区域，编码器能学习得比较好，会更有<span class="math inline">\(g({\bf x}) \approx {\bf x}\)</span>。那么对应的，如果对于新的数据有<span class="math inline">\(g({\bf x}_n) \approx {\bf x}_n\)</span>，就说明它落在了密度大的区域。如果要做离群点检测，就可以通过<span class="math inline">\(g({\bf x}_n) \not\approx {\bf x}_n\)</span>来找出离群点。也就是说，学习数据的隐藏特征/结构，也就是在学习什么样的数据是“典型的”数据，自动编码器可以判断数据典型与否</p>
<p>综上所述，自动编码器的目标看似只是学好恒等函数，实际它只是表象。重要意义是看隐含层，看数据有效的表示方式</p>
<p>如前面所述，自动编码器就是一个结构为<span class="math inline">\(d-\tilde{d}-d\)</span>，使用<span class="math inline">\(\sum_{i=1}^d (g_i({\bf x}) - x_i)^2\)</span>做误差函数的神经网络。它是一种浅层神经网络，因此容易训练。而且，通常<span class="math inline">\(\tilde{d} &lt; d\)</span>，以保证学到的是原始数据的<em>压缩</em>表示。构造的数据集是<span class="math inline">\(\{({\bf x}_1, {\bf y}_1 = {\bf x}_1), ({\bf x}_2, {\bf y}_2 = {\bf x}_2), \ldots, ({\bf x}_N, {\bf y}_N = {\bf x}_N)\}\)</span>（不看数据集中的标签，因此通常这个学习过程被看做是无监督学习的过程）。有时，还会加入限制条件<span class="math inline">\(w_{ij}^{(1)} = w_{ji}^{(2)}\)</span>做正则化，不过这样会使最后计算梯度的过程更加复杂</p>
<p>所以，回到上节最后，讲述深度学习如何初始化权重的方法。这个方法里提到说，初始化方法最有效的一招是每一层做一个预训练，这个预训练的过程就是用前一层的输出（对于第一层，是用输入数据）<span class="math inline">\(\{ {\bf x}_n^{(\ell -1)}\}\)</span>训练一个自动编码器，该自动编码器的隐藏层神经元数<span class="math inline">\(\tilde{d}\)</span>与深度神经网络中第<span class="math inline">\(\ell\)</span>层的节点个数<span class="math inline">\(d^{(\ell)}\)</span>相等</p>
<p>使用不同的体系结构加上不同的正则化，可以得到更精巧的自动编码器，进而得到更好的预训练结果。不过其本质都是相同的</p>
<h2 id="降噪自动编码器">降噪自动编码器</h2>
<p>前面讲述了如何在深度学习里做权重的预训练，这一节主要看一看如何加入一些独有的正则化技巧。前面提到过，深度学习里用到的正则化方法包括对结构做出一些限制、加入与权重有关的正则项以及提早结束训练等。但是考虑过拟合的成因，如果数据中噪声太大，也会造成过拟合。因此还可以再独辟蹊径，找到一种方法去降低噪声，这样也可以降低过拟合的风险。最简单的方法是直接做数据清洗，不过一种更疯狂的方法是向数据里加入噪声</p>
<p>在上一节里，自动编码器逼近的是恒等函数，即使得<span class="math inline">\(g({\bf x}) \approx {\bf x}\)</span>。但是一个足够鲁棒的自动编码器不仅能做到这一点，对于与输入有些微不同的<span class="math inline">\(\tilde{\bf x}\)</span>，还能做到<span class="math inline">\(g(\tilde{\bf x}) \approx {\bf x}\)</span>。也就是说，送进干净的数据可以产生干净的数据，而送进脏数据也可以出来干净的数据。这意味着足够鲁棒的自动编码器还能起到降噪，清洗数据的功能。因此，训练编码器时，可以故意往训练数据里加入一些人工噪声。即此时输入数据为<span class="math inline">\(\{(\tilde{\bf x}_1, {\bf y}_1 = {\bf x}_1), (\tilde{\bf x}_1, {\bf y}_2 = {\bf x}_2), \ldots, (\tilde{\bf x}_N, {\bf y}_N = {\bf x}_N)\}\)</span>，其中<span class="math inline">\(\tilde{\bf x}_n = {\bf x}_n + 人工噪声\)</span>。通过这种方法训练出来的自动编码器一般称为<strong>降噪自动编码器</strong>，在深度学习中通常是使用它，而不是原始的自动编码器。加入人工噪声这种方法告诉了算法我们所需要的性质，得到的模型对噪声容忍度有了很大的增强，因此这种手段也可以看作是一种正则化机制，可以适用到其它模型训练的过程中</p>
<h2 id="主成分分析pca">主成分分析（PCA）</h2>
<p>前面提到的自动编码器实际上是一种非线性模型，但是前面的课程一直都在说模型应该从最简单的，线性的模型试起，为什么这次反其道而行之了？原因是线性编码器是在深度学习预训练权重的背景下提出，而深度学习本身就是一个非线性的问题，因此在这个背景下直接讨论非线性的解决方案更有意义</p>
<p>当然，自动编码器也存在其线性形式，而且线性的自动编码器也有很多适用场合，因此仍然有研究的必要。线性自动编码器类似于前面提到的非线性自动编码器，只不过在对输入<span class="math inline">\(\bf x\)</span>求出得分以后不需要再做非线性变化<span class="math inline">\(\tanh\)</span>。这样一来，对<span class="math inline">\(\bf x\)</span>的第<span class="math inline">\(k\)</span>个维度，其假设函数形式可以写为 <span class="math display">\[
h_k({\bf x}) = \sum_{j=0}^{\tilde{d}}w_{kj}\left(\sum_{i=1}^d w_{ij}x_i\right)
\]</span> 注意内层求和项的指标是从1开始计数，因为对常数项逼近一个恒等函数没什么意义。此外，这里加入了前面所提到的正则化手段，即编码器和解码器的权重相等，<span class="math inline">\(w_{ij}^{(1)} = w_{ji}^{(2)}\)</span>，这里统一写成了<span class="math inline">\(w_{ij}\)</span>。以及，为了避免产生非平凡解，且考虑到实际要得到的是原始数据的<em>压缩</em>表示，还限制编码后数据的维度<span class="math inline">\(\tilde{d}\)</span>要小于原始维度<span class="math inline">\(d\)</span>。这样，记所有权重组成的矩阵为<span class="math inline">\(\rm W\)</span>，线性自动编码器假设函数的形式为 <span class="math display">\[
h({\bf x}) = {\rm WW}^\mathsf{T}{\bf x}
\]</span> 既然假设函数的形式已经被定义好，那么接下来的做法似乎水到渠成：只需要按部就班写出损失函数，就可以让损失函数对<span class="math inline">\(\rm W\)</span>求偏导，找出最优的<span class="math inline">\(\rm W\)</span>。损失函数的形式为（由于最后求出来的是一个向量，因此下面公式中用了黑体<span class="math inline">\(\bf h\)</span>） <span class="math display">\[
E_{\rm in}({\bf h})= E_{\rm in}({\rm W}) = \frac{1}{N}\sum_{n=1}^N \left\|{\bf x}_n - {\rm WW}^\mathsf{T}{\bf x}_n\right\|^2,\  \  {\rm W} \in \mathbb{R}^{d\times \tilde{d}}
\]</span> 前面讲到的线性问题都可以得到一个解析解，但是这里乍看之下并不容易，因为求和项里实际上是<span class="math inline">\(w_{ij}\)</span>的四次多项式，求解起来可能要费点周折，需要使用一些线性代数里的工具。由于<span class="math inline">\(\rm WW^{\mathsf{T}}\)</span>是半正定矩阵，因此可以对它做特征值分解，记为<span class="math inline">\(\rm WW^\mathsf{T} = V\Gamma V^{\mathsf{T}}\)</span>。这里得到的<span class="math inline">\(\rm V\)</span>是正交矩阵，即<span class="math inline">\({\rm VV^\mathsf{T} = V^\mathsf{T}V = I}_d\)</span>，而<span class="math inline">\(\rm \Gamma\)</span>是一个对角矩阵，由特征值组成，对角线上非零元素的数量至多为<span class="math inline">\(\tilde{d}\)</span>个。因此<span class="math inline">\({\rm WW^\mathsf{T}}{\bf x}_n = {\rm V\Gamma V}^\mathsf{T}{\bf x}_n\)</span>，接下来可通过优化<span class="math inline">\(\rm V\)</span>和<span class="math inline">\(\rm \Gamma\)</span>来最小化<span class="math inline">\(E_{\rm in}\)</span></p>
<p>在进入下一步之前，先看一下<span class="math inline">\({\rm WW^\mathsf{T}}{\bf x}_n = {\rm V\Gamma V}^\mathsf{T}{\bf x}_n\)</span>的几何意义。这里原数据左乘一个正交矩阵<span class="math inline">\({\rm V}^\mathsf{T}{\bf x}_n\)</span>实际上就是对原数据做一个坐标变换，类似于某种旋转或镜像。再左乘<span class="math inline">\(\rm \Gamma\)</span>，由于<span class="math inline">\(\rm \Gamma\)</span>是对角矩阵且只有至多<span class="math inline">\(\tilde{d}\)</span>个非零元素，因此这一步是将原数据至少<span class="math inline">\(d-\tilde{d}\)</span>个维度的分量置为0，然后对其他分量做缩放，得到一个新的坐标。最后再乘<span class="math inline">\(\rm V\)</span>，就是将新的坐标变换到原来的坐标系。因此，类似地，如果对原数据不做任何修改，那么有<span class="math inline">\({\bf x}_n = {\rm VIV^{\mathsf{T}}}{\bf x}_n\)</span>。原问题就变成了 <span class="math display">\[
\min_{\rm V} \min_{\rm \Gamma} \frac{1}{N}\sum_{n=1}^N\left\|{\rm VIV^{\mathsf{T}}}{\bf x}_n - {\rm V\Gamma V^{\mathsf{T}}}{\bf x}_n\right\|^2
\]</span> 首先来看最优的<span class="math inline">\(\rm \Gamma\)</span>。由于乘以<span class="math inline">\(\rm V\)</span>和<span class="math inline">\(\rm V^{\mathsf{T}}\)</span>是旋转和镜像，不改变向量的长度，因此去掉它对最优化问题的解没有影响。即上述问题等价于 <span class="math display">\[
\min_{\rm \Gamma}\sum\left\|({\rm I-\Gamma})({\rm some\  vector})\right\|^2
\]</span> 这里<span class="math inline">\(\rm \Gamma\)</span>是唯一的变量，因此为了让上式小，就要让<span class="math inline">\(\rm I-\Gamma\)</span>中有尽量多的0。由于<span class="math inline">\(\rm \Gamma\)</span>是对角矩阵，<span class="math inline">\(\rm I\)</span>是单位矩阵，那么为了达到效果，就要让<span class="math inline">\(\rm \Gamma\)</span>的对角线上有尽量多的1。由前面的推导，<span class="math inline">\(\rm \Gamma\)</span>上最多有<span class="math inline">\(\tilde{d}\)</span>个1。不失一般性地，最优的<span class="math inline">\(\rm \Gamma\)</span>可以为 <span class="math display">\[
{\rm \Gamma} = \left[\begin{matrix}{\rm I}_{\tilde{d}} &amp; 0 \\ 0 &amp; 0\end{matrix}\right]
\]</span> 这样，求解原问题等价于求解如下问题 <span class="math display">\[
\min_{\rm V}\sum_{n=1}^N \left\| \left[\begin{matrix}0 &amp; 0 \\ 0 &amp; {\rm I}_{d-\tilde{d}}\end{matrix}\right]{\rm V}^{\mathsf{T}}{\bf x}_n\right\|^2
\]</span> 直观地看，这个问题是在问“对向量<span class="math inline">\({\rm V}^\mathsf{T}{\bf x}_n\)</span>，留下哪几个分量可以使剩下的向量范数最小”。由于原向量是固定的，因此实际上这个问题等价于“对向量<span class="math inline">\({\rm V}^\mathsf{T}{\bf x}_n\)</span>，拿走哪几个分量可以使拿走的向量范数最大”。也就是说，有 <span class="math display">\[
\min_{\rm V}\sum_{n=1}^N \left\| \left[\begin{matrix}0 &amp; 0 \\ 0 &amp; {\rm I}_{d-\tilde{d}}\end{matrix}\right]{\rm V}^{\mathsf{T}}{\bf x}_n\right\|^2 \equiv \max_{\rm V}\sum_{n=1}^N \left\| \left[\begin{matrix} {\rm I}_{\tilde{d}} &amp; 0 \\ 0 &amp; 0\end{matrix}\right]{\rm V}^{\mathsf{T}}{\bf x}_n\right\|^2
\]</span> 考虑极端的情况，如果<span class="math inline">\(\tilde{d} = 1\)</span>，此时只与<span class="math inline">\(\rm V^\mathsf{T}\)</span>的第一个行向量<span class="math inline">\({\bf v}^\mathsf{T}\)</span>有关。考虑<span class="math inline">\(\rm V\)</span>是正交矩阵，有<span class="math inline">\({\bf v}^\mathsf{T}{\bf v} = 1\)</span>。因此可以得到如下最优化问题 <span class="math display">\[
\begin{align*}
\max_{\bf v}\  \  &amp;\sum_{n=1}^N {\bf v}^\mathsf{T}{\bf x}_n{\bf x}_n^\mathsf{T}{\bf v} \\
{\rm s.t.}\  \  &amp;{\bf v}^\mathsf{T}{\bf v} = 1
\end{align*}
\]</span> 使用拉格朗日乘子，可知最优的<span class="math inline">\(\bf v\)</span>满足 <span class="math display">\[
\sum_{n=1}^N {\bf x}_n{\bf x}_n^\mathsf{T} {\bf v} = \lambda {\bf v}
\]</span> （推导过程：引入拉格朗日乘子<span class="math inline">\(\lambda\)</span>，可知优化问题转为<span class="math inline">\(\mathcal{L} = \sum {\bf v}^\mathsf{T}{\bf x}_n{\bf x}_n^\mathsf{T}{\bf v} - \lambda(1-{\bf v}^\mathsf{T}{\bf v})\)</span>。<span class="math inline">\(\nabla \mathcal{L}_{\bf v} = 2\sum{\bf x}_n{\bf x}_n^\mathsf{T}{\bf v} - 2\lambda{\bf v}\)</span>，令<span class="math inline">\(\nabla \mathcal{L}_{\bf v} = 0\)</span>就可以得到上面这个关系）</p>
<p>而<span class="math inline">\(\sum_{n=1}^N {\bf x}_n{\bf x}_n^{\mathsf{T}}\)</span>是一个矩阵，如果记为<span class="math inline">\({\rm X^\mathsf{T}X}\)</span>，则根据特征值与特征向量的定义，最优的<span class="math inline">\(\bf v\)</span>实际上就是<span class="math inline">\({\rm X^\mathsf{T}X}\)</span>的第一个特征向量。推而广之，最优的<span class="math inline">\({\rm V} = \{ {\bf v}_j\}_{j=1}^{\tilde{d}}\)</span> 就是<span class="math inline">\({\rm X^\mathsf{T}X}\)</span>的前<span class="math inline">\(\tilde{d}\)</span>个特征向量</p>
<p>因此，线性自动编码器就是要把<span class="math inline">\(\rm X^\mathsf{T}X\)</span>求出来，得到前<span class="math inline">\(\tilde{d}\)</span>个特征向量的方向，就是应该投影，做特征变换的方向。算法如下</p>
<blockquote>
<p>计算<span class="math inline">\(\rm X^\mathsf{T}X\)</span>的前<span class="math inline">\(\tilde{d}\)</span>个特征向量<span class="math inline">\({\bf w}_1, {\bf w}_2, \ldots, {\bf w}_{\tilde{d}}\)</span></p>
<p>做特征变换<span class="math inline">\(\boldsymbol{\Phi}({\bf x}) = {\rm W}({\bf x})\)</span></p>
</blockquote>
<p>由上面的推导，可知线性编码器保证输入在投影后的空间里范数总和最大。如果换一个说法，说输入在投影后的空间里方差（变化量）总和最大，那么就是统计学里<strong>主成分分析</strong> (PCA) 的做法。PCA算法与线性自动编码器算法大同小异，不过由于其关注的是变化量总和，而变化量实际上是数据相对于平均数的差的平方，因此需要先把数据做一个相对于平均值的变换。PCA算法总体过程如下</p>
<blockquote>
<p>令<span class="math inline">\(\bar{\bf x} = \frac{1}{N}\sum_{n=1}^N {\bf x}_n\)</span>，令<span class="math inline">\({\bf x}_n \leftarrow {\bf x}_n - \bar{\bf x}\)</span></p>
<p>计算<span class="math inline">\(\rm X^\mathsf{T}X\)</span>的前<span class="math inline">\(\tilde{d}\)</span>个特征向量<span class="math inline">\({\bf w}_1, {\bf w}_2, \ldots, {\bf w}_{\tilde{d}}\)</span></p>
<p>做特征变换<span class="math inline">\(\boldsymbol{\Phi}({\bf x}) = {\rm W}({\bf x -\bar{x}})\)</span></p>
</blockquote>
<p>自动线性编码器和PCA都是非常好用的线性维度缩减的工具，可以用来处理数据。不过实践中PCA更常用一些</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Tingxun Shi 微信支付"/>
        <p>微信支付</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Tingxun Shi
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="txshi-mt.com/2017/10/21/NTUML-29-Deep-Learning/" title="NTUML 29. 深度学习">txshi-mt.com/2017/10/21/NTUML-29-Deep-Learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NTUML/" rel="tag"># NTUML</a>
          
            <a href="/tags/自动编码器/" rel="tag"># 自动编码器</a>
          
            <a href="/tags/PCA/" rel="tag"># PCA</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/10/19/NTUML-28-Neural-Network/" rel="next" title="NTUML 28. 神经网络">
                <i class="fa fa-chevron-left"></i> NTUML 28. 神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/10/22/NTUML-30-RBF-Network/" rel="prev" title="NTUML 30. 径向基函数网络">
                NTUML 30. 径向基函数网络 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="uyan_frame"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Tingxun Shi" />
            
              <p class="site-author-name" itemprop="name">Tingxun Shi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">77</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">73</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/timsonshi" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-globe"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="sakigami-yang.me" title="咲神" target="_blank">咲神</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度神经网络"><span class="nav-number">1.</span> <span class="nav-text">&#x6DF1;&#x5EA6;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自动编码器"><span class="nav-number">2.</span> <span class="nav-text">&#x81EA;&#x52A8;&#x7F16;&#x7801;&#x5668;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#降噪自动编码器"><span class="nav-number">3.</span> <span class="nav-text">&#x964D;&#x566A;&#x81EA;&#x52A8;&#x7F16;&#x7801;&#x5668;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#主成分分析pca"><span class="nav-number">4.</span> <span class="nav-text">&#x4E3B;&#x6210;&#x5206;&#x5206;&#x6790;&#xFF08;PCA&#xFF09;</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tingxun Shi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  
    

    
      <!-- UY BEGIN -->
      <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2146483"></script>
      <!-- UY END -->
    
  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
